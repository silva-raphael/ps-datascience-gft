{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244117c0",
   "metadata": {},
   "source": [
    "# Exercício 1 – Teórico: Fundamentos de RAG\n",
    "\n",
    "Explique, com suas próprias palavras:\n",
    "\n",
    "1. O que é RAG (Retrieval-Augmented Generation) e qual problema ele resolve na geração de texto usando modelos de linguagem?\n",
    "2. Quais são as vantagens de utilizar RAG em vez de depender apenas do modelo base?\n",
    "3. Quais são as principais etapas de um pipeline de RAG? Explique brevemente cada uma das seguintes fases:\n",
    "- Chunking\n",
    "- Indexing\n",
    "- Retrieval\n",
    "- Reranking\n",
    "- Geração (Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0badfd",
   "metadata": {},
   "source": [
    "# Exercício 2 – Prático: Chunking de Documentos\n",
    "\n",
    "Implemente uma função que realiza o *chunking* de um documento de texto longo.\n",
    "\n",
    "Requisitos:\n",
    "- A função deve dividir o texto em pedaços (chunks) de tamanho configurável (ex.: 500 tokens ou 1000 caracteres).\n",
    "- Deve permitir uma sobreposição (*overlap*) entre os chunks (ex.: 20% de sobreposição).\n",
    "- O output esperado é uma lista de chunks.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "chunks = chunk_text(long_text, chunk_size=500, overlap=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75856b2",
   "metadata": {},
   "source": [
    "# Exercício 3 – Prático: Indexação Vetorial\n",
    "\n",
    "Com os chunks criados no exercício anterior:\n",
    "\n",
    "1. Gere embeddings para cada chunk usando um modelo de embeddings (ex.: `sentence-transformers`, OpenAI embeddings ou outro de sua escolha).\n",
    "2. Crie um índice vetorial (FAISS, ChromaDB, Elasticsearch, etc.).\n",
    "3. Implemente uma função para realizar buscas no índice, retornando os Top-K documentos mais similares a uma query.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "results = search_index(query=\"Qual o impacto da inflação?\", top_k=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d365611",
   "metadata": {},
   "source": [
    "# Exercício 4 – Prático: Recuperação de Contexto + Geração de Resposta\n",
    "\n",
    "Implemente uma função que faça o seguinte:\n",
    "\n",
    "1. Receba uma query do usuário.\n",
    "2. Recupere os Top-K chunks mais relevantes do índice vetorial.\n",
    "3. Monte um *prompt* contendo a query + os textos dos chunks recuperados.\n",
    "4. Envie o prompt para um LLM (ex.: OpenAI, Mistral, Llama ou outro) para gerar uma resposta.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "response = rag_pipeline(query=\"Explique o conceito de inflação\")\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8611a",
   "metadata": {},
   "source": [
    "# Exercício 5 – Prático: Reranking\n",
    "\n",
    "Implemente uma etapa de reranking para melhorar a qualidade dos documentos recuperados:\n",
    "\n",
    "1. Após o retrieval inicial (ex.: Top-10), use um modelo de reranking (ex.: um `cross-encoder` da `sentence-transformers`) para reordenar os documentos com base na relevância para a query.\n",
    "2. Compare as respostas geradas pelo RAG antes e depois do reranking.\n",
    "3. Discuta brevemente se o reranking trouxe melhorias na qualidade da resposta.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "ranked_results = rerank(query, retrieved_docs)\n",
    "response = generate_answer(query, ranked_results)\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}